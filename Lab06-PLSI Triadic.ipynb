{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1SMms854DIn9MyC/c1jW/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"id":"99QhMRI5IuPu","executionInfo":{"status":"ok","timestamp":1762708187718,"user_tz":-330,"elapsed":9,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","from collections import defaultdict\n","import pickle"]},{"cell_type":"code","source":["from datasets import load_dataset\n","import pandas as pd\n","\n","# Load a small subset (Books category, English reviews)\n","dataset = load_dataset(\"amazon_polarity\", split=\"train[:5%]\")  # only 5% for speed\n","\n","# Convert to Pandas DataFrame\n","df = pd.DataFrame(dataset)\n","\n","# Rename columns to expected names\n","df = df.rename(columns={\"title\": \"asin\", \"content\": \"reviewText\"})\n","df[\"reviewerID\"] = [\"user_\" + str(i % 5000) for i in range(len(df))]  # simulate ~5k users\n","\n","# Keep only required columns\n","df = df[[\"reviewerID\", \"asin\", \"reviewText\"]].dropna()\n","\n","print(\"✅ Loaded dataset automatically\")\n","print(\"Number of reviews:\", len(df))\n","print(df.head())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ev0t0E6I17o","executionInfo":{"status":"ok","timestamp":1762708196454,"user_tz":-330,"elapsed":8730,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"d06fa66a-1af1-4101-c7e7-5659e28c44ec"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Loaded dataset automatically\n","Number of reviews: 180000\n","  reviewerID                                               asin  \\\n","0     user_0                     Stuning even for the non-gamer   \n","1     user_1              The best soundtrack ever to anything.   \n","2     user_2                                           Amazing!   \n","3     user_3                               Excellent Soundtrack   \n","4     user_4  Remember, Pull Your Jaw Off The Floor After He...   \n","\n","                                          reviewText  \n","0  This sound track was beautiful! It paints the ...  \n","1  I'm reading a lot of reviews saying that this ...  \n","2  This soundtrack is my favorite music of all ti...  \n","3  I truly like this soundtrack and I enjoy video...  \n","4  If you've played the game, you know how divine...  \n"]}]},{"cell_type":"code","source":["# Keep users and documents with minimum activity\n","min_reviews_user = 2\n","min_reviews_doc = 2\n","\n","# Count user & doc frequencies\n","user_counts = df['reviewerID'].value_counts()\n","doc_counts = df['asin'].value_counts()\n","\n","users_keep = user_counts[user_counts>=min_reviews_user].index\n","docs_keep  = doc_counts[doc_counts>=min_reviews_doc].index\n","\n","df2 = df[df['reviewerID'].isin(users_keep) & df['asin'].isin(docs_keep)].copy()\n","print(\"Filtered reviews:\", len(df2))\n","\n","# Relabel user_ids and doc_ids for indices\n","user2idx = {u:i for i,u in enumerate(df2['reviewerID'].unique())}\n","doc2idx  = {d:i for i,d in enumerate(df2['asin'].unique())}\n","\n","df2['u_idx'] = df2['reviewerID'].map(user2idx)\n","df2['d_idx'] = df2['asin'].map(doc2idx)\n","\n","U = len(user2idx)\n","D = len(doc2idx)\n","print(\"Num users:\", U, \"Num documents:\", D)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qRK4IVAI-6-","executionInfo":{"status":"ok","timestamp":1762708196729,"user_tz":-330,"elapsed":242,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"ac59ecd6-3e81-4ef3-9813-e9e56cdb5ddd"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered reviews: 34846\n","Num users: 4997 Num documents: 7336\n"]}]},{"cell_type":"code","source":["import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# Download stopwords if not already present\n","try:\n","    stopwords = stopwords.words('english')\n","except LookupError:\n","    nltk.download('stopwords')\n","    stopwords = stopwords.words('english')\n","\n","\n","# Basic tokenization and cleaning with stop word removal\n","def preprocess(text):\n","    text = text.lower()\n","    # Remove non-alphanumeric characters and punctuation\n","    text = re.sub(r'[^a-z0-9\\s]', '', text)\n","    tokens = text.split()\n","    # Remove stop words\n","    tokens = [word for word in tokens if word not in stopwords]\n","    return tokens\n","\n","# Build vocabulary from reviewText of df2\n","all_texts = df2['reviewText'].tolist()\n","vectorizer = CountVectorizer(max_features=2000, tokenizer=preprocess)\n","X_counts = vectorizer.fit_transform(all_texts)\n","vocab = vectorizer.get_feature_names_out()\n","W = len(vocab)\n","print(\"Vocabulary size:\", W)\n","\n","word2idx = {w:i for i,w in enumerate(vocab)}\n","\n","# Build triplets (u, d, w, count)\n","triplet_counts = defaultdict(int)\n","for idx, row in df2.iterrows():\n","    u = row['u_idx']\n","    d = row['d_idx']\n","    text = row['reviewText']\n","    tokens = preprocess(text)\n","    for t in tokens:\n","        if t in word2idx:\n","            w = word2idx[t]\n","            triplet_counts[(u,d,w)] += 1\n","\n","triplets = [(u, d, w, c) for (u,d,w),c in triplet_counts.items() if c>0]\n","print(\"Num non-zero triplets:\", len(triplets))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CK7R0isOJCGG","executionInfo":{"status":"ok","timestamp":1762708212117,"user_tz":-330,"elapsed":15343,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"18884367-3f9e-4222-ad0f-40b42b3af2e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Vocabulary size: 2000\n","Num non-zero triplets: 737937\n"]}]},{"cell_type":"code","source":["# Shuffle and split\n","random.shuffle(triplets)\n","split = int(0.9 * len(triplets))\n","train_triplets = triplets[:split]\n","test_triplets  = triplets[split:]\n","print(\"Train size:\", len(train_triplets), \"Test size:\", len(test_triplets))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9Kx5Q5nJF_2","executionInfo":{"status":"ok","timestamp":1762708212449,"user_tz":-330,"elapsed":329,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"5ae4fc9c-398f-42d2-88c1-9a6cd4708674"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Train size: 664143 Test size: 73794\n"]}]},{"cell_type":"code","source":["def train_triadic_plsi(train_triplets, U, D, W, K=10, max_iter=30, eps=1e-12):\n","    # Initialize\n","    Pz_u = np.random.rand(U, K)\n","    Pz_u /= Pz_u.sum(axis=1, keepdims=True)\n","\n","    Pd_z = np.random.rand(K, D)\n","    Pd_z /= Pd_z.sum(axis=1, keepdims=True)\n","\n","    Pw_z = np.random.rand(K, W)\n","    Pw_z /= Pw_z.sum(axis=1, keepdims=True)\n","\n","    for it in range(max_iter):\n","        # E-step: compute q(z|u,d,w) for each triplet\n","        Q = []\n","        for (u,d,w,c) in train_triplets:\n","            val = Pz_u[u, :] * Pd_z[:, d] * Pw_z[:, w]\n","            s   = val.sum()\n","            if s < eps:\n","                q = np.ones(K) / K\n","            else:\n","                q = val / s\n","            Q.append((u,d,w,c,q))\n","\n","        # M-step: update counts\n","        Pz_u_new = np.zeros_like(Pz_u)\n","        Pd_z_new = np.zeros_like(Pd_z)\n","        Pw_z_new = np.zeros_like(Pw_z)\n","\n","        for (u,d,w,c,q) in Q:\n","            Pz_u_new[u, :] += c * q\n","            Pd_z_new[:, d] += c * q\n","            Pw_z_new[:, w] += c * q\n","\n","        # Normalize\n","        Pz_u = (Pz_u_new.T / Pz_u_new.sum(axis=1)).T\n","        Pd_z = (Pd_z_new.T / Pd_z_new.sum(axis=1)).T\n","        Pw_z = (Pw_z_new.T / Pw_z_new.sum(axis=1)).T\n","\n","        # Compute log-likelihood on train\n","        ll = 0.0\n","        for (u,d,w,c) in train_triplets:\n","            prob = np.sum(Pz_u[u, :] * Pd_z[:, d] * Pw_z[:, w])\n","            ll += c * np.log(prob + eps)\n","        print(f\"Iter {it+1}/{max_iter}, train log-likelihood = {ll:.2f}\")\n","\n","    return Pz_u, Pd_z, Pw_z\n"],"metadata":{"id":"9HvUhWqRJLqO","executionInfo":{"status":"ok","timestamp":1762708212494,"user_tz":-330,"elapsed":11,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["K = 7\n","Pz_u, Pd_z, Pw_z = train_triadic_plsi(train_triplets, U, D, W, K=K, max_iter=50) # Increased max_iter to 50"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JgGREyZJOz9","executionInfo":{"status":"ok","timestamp":1762708997575,"user_tz":-330,"elapsed":785079,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"94563000-421c-481f-e634-fc4bf50db04a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 1/50, train log-likelihood = -11189043.50\n","Iter 2/50, train log-likelihood = -11116381.96\n","Iter 3/50, train log-likelihood = -11027043.86\n","Iter 4/50, train log-likelihood = -10926509.89\n","Iter 5/50, train log-likelihood = -10830524.61\n","Iter 6/50, train log-likelihood = -10752825.55\n","Iter 7/50, train log-likelihood = -10697832.47\n","Iter 8/50, train log-likelihood = -10661004.68\n","Iter 9/50, train log-likelihood = -10635685.81\n","Iter 10/50, train log-likelihood = -10617497.43\n","Iter 11/50, train log-likelihood = -10604014.40\n","Iter 12/50, train log-likelihood = -10593884.99\n","Iter 13/50, train log-likelihood = -10586065.35\n","Iter 14/50, train log-likelihood = -10579673.85\n","Iter 15/50, train log-likelihood = -10574304.94\n","Iter 16/50, train log-likelihood = -10569813.57\n","Iter 17/50, train log-likelihood = -10566008.67\n","Iter 18/50, train log-likelihood = -10562710.11\n","Iter 19/50, train log-likelihood = -10559813.63\n","Iter 20/50, train log-likelihood = -10557223.05\n","Iter 21/50, train log-likelihood = -10554904.82\n","Iter 22/50, train log-likelihood = -10552842.84\n","Iter 23/50, train log-likelihood = -10551019.03\n","Iter 24/50, train log-likelihood = -10549396.99\n","Iter 25/50, train log-likelihood = -10547936.28\n","Iter 26/50, train log-likelihood = -10546616.49\n","Iter 27/50, train log-likelihood = -10545379.32\n","Iter 28/50, train log-likelihood = -10544187.63\n","Iter 29/50, train log-likelihood = -10543063.53\n","Iter 30/50, train log-likelihood = -10542037.57\n","Iter 31/50, train log-likelihood = -10541100.87\n","Iter 32/50, train log-likelihood = -10540230.36\n","Iter 33/50, train log-likelihood = -10539421.76\n","Iter 34/50, train log-likelihood = -10538680.39\n","Iter 35/50, train log-likelihood = -10537984.17\n","Iter 36/50, train log-likelihood = -10537305.54\n","Iter 37/50, train log-likelihood = -10536658.24\n","Iter 38/50, train log-likelihood = -10536045.02\n","Iter 39/50, train log-likelihood = -10535442.88\n","Iter 40/50, train log-likelihood = -10534855.34\n","Iter 41/50, train log-likelihood = -10534308.08\n","Iter 42/50, train log-likelihood = -10533794.62\n","Iter 43/50, train log-likelihood = -10533306.54\n","Iter 44/50, train log-likelihood = -10532843.72\n","Iter 45/50, train log-likelihood = -10532394.43\n","Iter 46/50, train log-likelihood = -10531940.77\n","Iter 47/50, train log-likelihood = -10531489.21\n","Iter 48/50, train log-likelihood = -10531035.09\n","Iter 49/50, train log-likelihood = -10530590.04\n","Iter 50/50, train log-likelihood = -10530190.67\n"]}]},{"cell_type":"code","source":["def compute_perplexity(triplets, Pz_u, Pd_z, Pw_z, eps=1e-12):\n","    N = len(triplets)\n","    ll = 0.0\n","    for (u,d,w,c) in triplets:\n","        prob = np.sum(Pz_u[u, :] * Pd_z[:, d] * Pw_z[:, w])\n","        ll   += c * np.log(prob + eps)\n","    perp = np.exp(-ll / N)\n","    return ll, perp\n","\n","ll_test, perp_test = compute_perplexity(test_triplets, Pz_u, Pd_z, Pw_z)\n","print(\"Test log-likelihood = %.2f, perplexity = %.2f\" % (ll_test, perp_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0TE9UleJSAV","executionInfo":{"status":"ok","timestamp":1762708998582,"user_tz":-330,"elapsed":1005,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"4369c37e-6b4e-4546-acf3-51953b9cf5ab"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Test log-likelihood = -1174206.73, perplexity = 8137178.03\n"]}]},{"cell_type":"code","source":["# Top unique words per topic\n","top_n = 10\n","for z in range(K):\n","    # Calculate the probability of a word given a topic divided by the sum of probabilities of that word across all topics\n","    # This highlights words that are more unique to a topic\n","    word_uniqueness = Pw_z[z, :] / np.sum(Pw_z, axis=0)\n","    top_w_idx = np.argsort(word_uniqueness)[-top_n:]\n","    top_words   = [vocab[w] for w in top_w_idx]\n","    print(f\"Topic {z}: {top_words}\")\n","\n","# Sample a user and inspect user’s topic distribution\n","u_sample = 0\n","print(\"User\", u_sample, \"topic distribution:\", Pz_u[u_sample,:].round(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_C9z8P9JUe1","executionInfo":{"status":"ok","timestamp":1762708998602,"user_tz":-330,"elapsed":15,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"cb926735-926a-4a79-f093-d3f7c487fc9b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 0: ['ipod', 'card', 'wire', 'wood', 'diaper', 'cutting', 'claims', 'oil', 'images', 'footage']\n","Topic 1: ['practical', 'ice', 'pc', 'elizabeth', 'luck', 'johnny', 'tiny', 'mirror', 'ii', 'letter']\n","Topic 2: ['potter', 'rack', 'rice', 'teaching', 'oxo', 'rush', 'la', 'r', 'de', 'hobbit']\n","Topic 3: ['3d', 'classics', 'reach', 'discovered', 'ahead', 'disney', 'nicely', 'usb', 'remove', 'explanation']\n","Topic 4: ['brush', 'shut', 'steps', 'ice', 'situations', 'publisher', 'killing', 'network', 'terrific', 'johnny']\n","Topic 5: ['roll', 'wind', 'brian', 'workout', 'plane', 'receiver', 'training', 'community', 'giver', 'jonas']\n","Topic 6: ['anne', 'everyday', 'frankly', 'flow', 'comic', 'concerned', 'exam', 'mirror', 'avid', 'dust']\n","User 0 topic distribution: [0.623 0.    0.377 0.    0.    0.    0.   ]\n"]}]},{"cell_type":"code","source":["# Build doc-word counts\n","dw_counts = defaultdict(int)\n","for (u,d,w,c) in train_triplets:\n","    dw_counts[(d,w)] += c\n","dw_triplets = [(d,w,c) for (d,w),c in dw_counts.items()]\n","\n","def train_plsi_doc_word(dw_triplets, D, W, K=20, max_iter=20, eps=1e-12):\n","    Pz_d = np.random.rand(D, K)\n","    Pz_d /= Pz_d.sum(axis=1, keepdims=True)\n","    Pw_z  = np.random.rand(K, W)\n","    Pw_z  /= Pw_z.sum(axis=1, keepdims=True)\n","\n","    for it in range(max_iter):\n","        Q = []\n","        for (d,w,c) in dw_triplets:\n","            val = Pz_d[d, :] * Pw_z[:, w]\n","            s   = val.sum()\n","            if s < eps:\n","                q = np.ones(K) / K\n","            else:\n","                q = val / s\n","            Q.append((d,w,c,q))\n","\n","        Pz_d_new = np.zeros_like(Pz_d)\n","        Pw_z_new = np.zeros_like(Pw_z)\n","\n","        for (d,w,c,q) in Q:\n","            Pz_d_new[d, :] += c * q\n","            Pw_z_new[:, w]  += c * q\n","\n","        Pz_d = (Pz_d_new.T / (Pz_d_new.sum(axis=1) + eps)).T # Added epsilon here\n","        Pw_z  = (Pw_z_new.T  / (Pw_z_new.sum(axis=1) + eps)).T # Added epsilon here\n","\n","        # log-likelihood\n","        ll = 0.0\n","        for (d,w,c) in dw_triplets:\n","            prob = np.sum(Pz_d[d, :] * Pw_z[:, w])\n","            ll += c * np.log(prob + eps)\n","        print(f\"Vanilla PLSI iter {it+1}/{max_iter}, ll = {ll:.2f}\")\n","    return Pz_d, Pw_z\n","\n","Pz_d_bw, Pw_z_bw = train_plsi_doc_word(dw_triplets, D, W, K=K, max_iter=20)\n","# Evaluate baseline perplexity\n","# build test doc-word triplets similarly:\n","dw_test_counts = defaultdict(int)\n","for (u,d,w,c) in test_triplets:\n","    dw_test_counts[(d,w)] += c\n","dw_test_triplets = [(d,w,c) for (d,w),c in dw_test_counts.items()]\n","# compute perplexity for baseline\n","ll_bw, perp_bw = None, None\n","# compute\n","ll_bw = 0.0\n","for (d,w,c) in dw_test_triplets:\n","    prob = np.sum(Pz_d_bw[d, :] * Pw_z_bw[:, w])\n","    ll_bw   += c * np.log(prob + 1e-12)\n","perp_bw = np.exp(- ll_bw / len(dw_test_triplets))\n","print(\"Baseline (vanilla PLSI) test perplexity = %.2f\" % perp_bw)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUyF59fYJXU9","executionInfo":{"status":"ok","timestamp":1762709184493,"user_tz":-330,"elapsed":185889,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"2ffb2214-13d3-4caa-b31d-828aef330ce2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Vanilla PLSI iter 1/20, ll = -5127030.75\n","Vanilla PLSI iter 2/20, ll = -5123302.43\n","Vanilla PLSI iter 3/20, ll = -5120115.96\n","Vanilla PLSI iter 4/20, ll = -5117117.70\n","Vanilla PLSI iter 5/20, ll = -5114062.91\n","Vanilla PLSI iter 6/20, ll = -5110741.12\n","Vanilla PLSI iter 7/20, ll = -5106934.30\n","Vanilla PLSI iter 8/20, ll = -5102393.54\n","Vanilla PLSI iter 9/20, ll = -5096840.01\n","Vanilla PLSI iter 10/20, ll = -5090004.55\n","Vanilla PLSI iter 11/20, ll = -5081724.31\n","Vanilla PLSI iter 12/20, ll = -5072087.46\n","Vanilla PLSI iter 13/20, ll = -5061535.24\n","Vanilla PLSI iter 14/20, ll = -5050777.45\n","Vanilla PLSI iter 15/20, ll = -5040521.68\n","Vanilla PLSI iter 16/20, ll = -5031232.41\n","Vanilla PLSI iter 17/20, ll = -5023080.93\n","Vanilla PLSI iter 18/20, ll = -5016038.86\n","Vanilla PLSI iter 19/20, ll = -5009994.90\n","Vanilla PLSI iter 20/20, ll = -5004823.06\n","Baseline (vanilla PLSI) test perplexity = 3138.24\n"]}]}]}