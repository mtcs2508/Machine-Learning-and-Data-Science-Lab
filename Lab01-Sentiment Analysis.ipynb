{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPoNs6yA8WGveW/+seFh5Gc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_8nneV93Px7","executionInfo":{"status":"ok","timestamp":1762703093373,"user_tz":-330,"elapsed":35405,"user":{"displayName":"navas roshan","userId":"12864937970405172599"}},"outputId":"b126c78e-99d6-4754-9648-b0f8aa0cdda0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset...\n","\n"," Loaded 1400 reviews (700 positive / 700 negative)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Model Metrics:\n","\n","             Features  #Features  NaiveBayes    LogReg       SVM\n","    (1) Unigrams freq      12960   78.713856       NaN 77.929162\n","(2) Unigrams presence      12960   81.356511 83.142483 81.642634\n","     (4) Bigrams only      15825   79.285948 77.857324 75.572016\n"," (3) Unigrams+Bigrams      24462   81.713552 82.571155 80.999776\n","  (5) Adjectives only       1925   76.069975 73.284411 69.927520\n","(6) Top 2633 unigrams       2633   80.855183 80.712734 78.427273\n"]}],"source":["import re, zipfile, urllib.request\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import cross_val_score\n","\n","\n","# Fetch & Extract Dataset\n","url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip\"\n","zip_file = Path(\"mix20_rand700_tokens_cleaned.zip\")\n","data_dir = Path(zip_file.stem)\n","\n","print(\"Downloading dataset...\")\n","urllib.request.urlretrieve(url, zip_file)\n","\n","with zipfile.ZipFile(zip_file, \"r\") as zf:\n","  zf.extractall(data_dir)\n","\n","\n","pos_path = data_dir / \"tokens\" / \"pos\"\n","neg_path = data_dir / \"tokens\" / \"neg\"\n","dataset_version = \"Polarity v0.9/v1.0 â€“ 700 pos / 700 neg\"\n","\n","\n","#Load Data\n","\n","def read_reviews(pos_dir, neg_dir):\n","    docs, y = [], []\n","    for lbl, folder in [(1, pos_dir), (0, neg_dir)]:\n","        for file in Path(folder).glob(\"*.txt\"):\n","            text = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n","            # strip ratings like \"10/10\" or \"****\"\n","            text = re.sub(r\"\\d+/\\d+|\\*+\", \"\", text)\n","            docs.append(text)\n","            y.append(lbl)\n","    return docs, np.array(y)\n","\n","reviews, labels = read_reviews(pos_path, neg_path)\n","print(f\"\\n Loaded {len(reviews)} reviews \"\n","      f\"({labels.sum()} positive / {(labels==0).sum()} negative)\")\n","\n","#Evaluation Helper\n","def cv_score(model, X, y, folds=3):\n","    return cross_val_score(model, X, y, cv=folds).mean() * 100\n","\n","#Feature Experiments\n","experiments = []\n","token_rule = r\"(?u)\\b\\w+\\b\"\n","\n","# (1) Unigrams (counts)\n","vec = CountVectorizer(binary=False, token_pattern=token_rule, min_df=4)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(1) Unigrams freq\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    None,\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","\n","# (2) Unigrams (binary presence)\n","vec = CountVectorizer(binary=True, token_pattern=token_rule, min_df=4)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(2) Unigrams presence\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","\n","#  (3) Bigrams only\n","vec = CountVectorizer(binary=True, ngram_range=(2,2),\n","                      token_pattern=token_rule, min_df=7)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(4) Bigrams only\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","#(4) Unigrams + Bigrams\n","vec = CountVectorizer(binary=True, ngram_range=(1,2),\n","                      token_pattern=token_rule, min_df=7)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(3) Unigrams+Bigrams\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","\n","\n","# (5) Adjective-based tokens\n","def adjective_filter(text):\n","    words = re.findall(r\"\\b\\w+\\b\", text)\n","    pattern = r\"(ly$|ous$|ful$|able$|ive$|less$|ic$|al$|est$|er$)\"\n","    keywords = {\"good\", \"bad\", \"great\", \"awful\", \"excellent\", \"poor\"}\n","    return [w for w in words if re.search(pattern, w.lower()) or w.lower() in keywords]\n","\n","vec = CountVectorizer(tokenizer=adjective_filter, binary=True, min_df=4)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(5) Adjectives only\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","\n","# (6) Top 2633 unigrams\n","vec = CountVectorizer(binary=True, token_pattern=token_rule, max_features=2633)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(6) Top 2633 unigrams\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","\n","#Printing Results\n","results_df = pd.DataFrame(experiments, columns=[\"Features\", \"#Features\", \"NaiveBayes\", \"LogReg\", \"SVM\"])\n","print(\"\\nModel Metrics:\\n\")\n","print(results_df.to_string(index=False))\n"]}]}