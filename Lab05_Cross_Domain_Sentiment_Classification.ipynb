{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX9PYiOY3SpQ"
      },
      "outputs": [],
      "source": [
        "import re, numpy as np, scipy.sparse as sp\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from datasets import load_dataset\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Tokenization & Preprocessing ---\n",
        "def simple_tokenize(t):\n",
        "    t = re.sub(r\"[^a-z0-9'\\s]\", \" \", t.lower())\n",
        "    return [w for w in t.split() if len(w) > 1]\n",
        "def preprocess(corpus):\n",
        "    return [simple_tokenize(x) for x in corpus]\n",
        "\n",
        "# --- Vocabulary & Domain Split ---\n",
        "def build_vocab(src, tgt):\n",
        "    s, t = Counter([w for d in src for w in d]), Counter([w for d in tgt for w in d])\n",
        "    return s, t, set(s) | set(t)\n",
        "\n",
        "def split_words(s, t, vocab, min_freq=5, ratio=5.0):\n",
        "    di, ss, ts = set(), set(), set()\n",
        "    for w in vocab:\n",
        "        fs, ft = s.get(w, 0), t.get(w, 0)\n",
        "        if fs + ft < min_freq: continue\n",
        "        if fs > 0 and ft > 0:\n",
        "            r = (fs + 1) / (ft + 1)\n",
        "            if 1/ratio <= r <= ratio: di.add(w)\n",
        "            elif r > ratio: ss.add(w)\n",
        "            else: ts.add(w)\n",
        "        elif fs > 0: ss.add(w)\n",
        "        else: ts.add(w)\n",
        "    return di, ss, ts\n",
        "\n",
        "# --- Co-occurrence Matrix ---\n",
        "def cooc(docs, spec, indep, win=5):\n",
        "    sl, il = sorted(spec), sorted(indep)\n",
        "    si, ii = {w:i for i,w in enumerate(sl)}, {w:i for i,w in enumerate(il)}\n",
        "    r, c, v = [], [], []\n",
        "    for doc in docs:\n",
        "        n = len(doc)\n",
        "        for i, w in enumerate(doc):\n",
        "            if w in si:\n",
        "                for u in doc[max(0, i - win):min(n, i + win + 1)]:\n",
        "                    if u in ii: r.append(si[w]); c.append(ii[u]); v.append(1)\n",
        "    return sp.csr_matrix((v, (r, c)), shape=(len(sl), len(il))), sl, il\n",
        "\n",
        "# --- Spectral Feature Alignment ---\n",
        "def sfa(M, dims=100):\n",
        "    if M.shape[0] == 0 or M.shape[1] == 0:\n",
        "        return np.zeros((M.shape[0], dims))\n",
        "    svd = TruncatedSVD(n_components=min(dims, min(M.shape)-1 or 1), random_state=42)\n",
        "    W = svd.fit_transform(M)\n",
        "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-8\n",
        "    return W\n",
        "\n",
        "# --- Word Embeddings ---\n",
        "def build_emb(spec, indep, W, M):\n",
        "    M_dense = M.toarray() if M.nnz > 0 else np.zeros((len(spec), len(indep)))\n",
        "    indep_emb = {w: ((W * (M_dense[:, j:j+1])).sum(0) / (M_dense[:, j:j+1].sum() or 1)) for j, w in enumerate(indep)}\n",
        "    spec_emb = {w: W[i] for i, w in enumerate(spec)}\n",
        "    return spec_emb, indep_emb\n",
        "\n",
        "# --- Document Vectorization ---\n",
        "def doc_vec(toks, spec_emb, indep_emb, dim=50):\n",
        "    vecs = [spec_emb[w] for w in toks if w in spec_emb] + [indep_emb[w] for w in toks if w in indep_emb]\n",
        "    return np.mean(vecs, 0) if vecs else np.zeros(dim)\n",
        "\n",
        "# --- Full SFA Pipeline ---\n",
        "def run_sfa(src_texts, src_labels, tgt_texts, tgt_labels, min_freq=5, dims=100, n=2000):\n",
        "    tok_src, tok_tgt = preprocess(src_texts[:n]), preprocess(tgt_texts[:n])\n",
        "    s, t, vocab = build_vocab(tok_src, tok_tgt)\n",
        "    di, ss, ts = split_words(s, t, vocab, min_freq)\n",
        "    all_docs = tok_src + tok_tgt\n",
        "\n",
        "    M_s, sl_s, indep = cooc(all_docs, ss, di)\n",
        "    M_t, sl_t, _ = cooc(all_docs, ts, di)\n",
        "    W = sfa(sp.vstack([M_s, M_t]), dims)\n",
        "    ns = M_s.shape[0]\n",
        "    W_s, W_t = W[:ns], W[ns:]\n",
        "\n",
        "    spec_s, indep_emb = build_emb(sl_s, indep, W_s, M_s)\n",
        "    spec_t, _ = build_emb(sl_t, indep, W_t, M_t)\n",
        "    spec_emb = {**spec_s, **spec_t}\n",
        "\n",
        "    dim = W.shape[1]\n",
        "    Xs = np.vstack([doc_vec(d, spec_emb, indep_emb, dim) for d in tok_src])\n",
        "    Xt = np.vstack([doc_vec(d, spec_emb, indep_emb, dim) for d in tok_tgt])\n",
        "\n",
        "    clf = LinearSVC(random_state=42, max_iter=5000).fit(Xs, src_labels[:n])\n",
        "    y_pred = clf.predict(Xt)\n",
        "\n",
        "    print(\"Accuracy:\", round(accuracy_score(tgt_labels[:n], y_pred), 4))\n",
        "    print(\"Macro-F1:\", round(f1_score(tgt_labels[:n], y_pred, average='macro'), 4))\n",
        "    print(classification_report(tgt_labels[:n], y_pred))\n",
        "\n",
        "# --- Run on Amazon â†’ Yelp ---\n",
        "amazon = load_dataset(\"amazon_polarity\", split=\"train[:5000]\")\n",
        "yelp = load_dataset(\"yelp_polarity\", split=\"train[:5000]\")\n",
        "run_sfa([x[\"content\"] for x in amazon], [x[\"label\"] for x in amazon],\n",
        "        [x[\"text\"] for x in yelp], [x[\"label\"] for x in yelp],\n",
        "        min_freq=3, dims=50, n=2000)\n"
      ]
    }
  ]
}